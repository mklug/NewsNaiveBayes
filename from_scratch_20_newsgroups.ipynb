{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an implementation of the multinomial naive Bayes classifier together with a unit test on synthetic data and a performance comparison with the multinomial naive Bayes model in sklearn on the 20 Newsgroups dataset.  The preformance of the model is essentially the same as the sklearn version with the small difference being a slight difference in preprocessing.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections # for Counter\n",
    "import math        # for log\n",
    "\n",
    "class MultinomialNaiveBayes(object):\n",
    "\n",
    "    def __init__(self, alpha=1.0, uniform_priors=False):\n",
    "        '''\n",
    "        The parameters ``theta_hat`` are stored in a matrix after training.  \n",
    "        Therefore, we need to assign indices to the classes (and have the \n",
    "        reverse mapping) and indices to the words -- hence the attributes\n",
    "        ``class_to_index``, ``index_to_class``, and ``word_to_index``.  \n",
    "\n",
    "        The priors are stored in an array ``priors`` after training \n",
    "        with the indices of the array matching the indices in \n",
    "        ``class_to_index``.  ``alpha`` is the smoothing parameter and \n",
    "        ``uniform_priors`` is a boolean that allows us to set the priors \n",
    "        to uniform.\n",
    "        '''\n",
    "        self.class_to_index = {}\n",
    "        self.index_to_class = {} # needed for prediction.\n",
    "        # list of the priors\n",
    "        self.priors = None\n",
    "        self.word_to_index = {}\n",
    "        # matrix with ci entry theta^hat_ci\n",
    "        self.theta_hat = None\n",
    "        self.alpha = alpha\n",
    "        self.uniform_priors=uniform_priors\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        '''\n",
    "        Input: ``X_train`` is a list of lists of strings and ``y_train`` is \n",
    "        a list of the same length as ``X_train`` of corresponding classes. \n",
    "        '''\n",
    "        assert len(X_train) == len(y_train)\n",
    "\n",
    "        N = len(y_train) # number of datapoints.\n",
    "\n",
    "        # assign indices to the classes.\n",
    "        class_counts = collections.Counter(y_train)\n",
    "        self.class_to_index = {c:i for i,c in enumerate(class_counts.keys())}\n",
    "        self.index_to_class = {v:k for k,v in self.class_to_index.items()}\n",
    "\n",
    "        # compute priors.\n",
    "        C  = len(self.class_to_index) # number of classes.  \n",
    "        self.priors = [1/C] * C\n",
    "        if not self.uniform_priors:\n",
    "            for c,count in class_counts.items():\n",
    "                self.priors[self.class_to_index[c]] = count / N\n",
    "\n",
    "        # compute the total word counts and set the indices.  \n",
    "        total_words_in_class = [0] * C # entries are N_c in the paper.\n",
    "        word_counts = collections.Counter()\n",
    "\n",
    "        for x, y in zip(X_train, y_train):\n",
    "            word_counts += collections.Counter(x)\n",
    "            total_words_in_class[self.class_to_index[y]] += len(x)\n",
    "\n",
    "        self.word_to_index = {word:i for i,word in enumerate(word_counts.keys())}        \n",
    "\n",
    "        # compute theta hats.  \n",
    "        # need N_ci from the paper. First we fill the matrix with them, then compute theta^hat_ci.\n",
    "        W = len(self.word_to_index) # number of words.  \n",
    "        self.theta_hat = [[0.0] * len(self.word_to_index) for _ in range(C)]\n",
    "    \n",
    "        for x, y in zip(X_train, y_train):\n",
    "            for word, count in collections.Counter(x).items():\n",
    "                self.theta_hat[self.class_to_index[y]][self.word_to_index[word]] += count    \n",
    "\n",
    "        for c in range(C):\n",
    "            for i in range(W):\n",
    "                self.theta_hat[c][i] = ((self.theta_hat[c][i] + self.alpha) / \n",
    "                                        (total_words_in_class[c] + W * self.alpha))\n",
    "\n",
    "    def _get_max_index(nums):\n",
    "        '''Returns the index of ``nums`` with the max value.'''\n",
    "        return max(range(len(nums)), key=nums.__getitem__)\n",
    "\n",
    "    def predict_log_proba(self, X_test):\n",
    "        '''Returns the raw log probabilities predicted by the model as \n",
    "        an array for an array of inputs.  \n",
    "        '''\n",
    "        res = []\n",
    "        for x in X_test:\n",
    "            # index is the same as in ``index_to_class``\n",
    "            log_liklihoods = [math.log(prior) for prior in self.priors]\n",
    "            C = len(self.class_to_index)\n",
    "            for word, count in collections.Counter(x).items():\n",
    "                for c in range(C):\n",
    "                    # skip the words that are new.\n",
    "                    if word in self.word_to_index:\n",
    "                        log_liklihoods[c] += count * math.log(self.theta_hat[c][self.word_to_index[word]])\n",
    "            res.append(log_liklihoods)\n",
    "        return res\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''Returns an the classes predicted by the trained model for\n",
    "        as an array corresponding the the array of inputs.\n",
    "        '''\n",
    "        log_probas = MultinomialNaiveBayes.predict_log_proba(self, X_test)\n",
    "        return [self.index_to_class[MultinomialNaiveBayes._get_max_index(lp)] for lp in log_probas]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells provide example usage for the ``MultinomialNaiveBayes`` class. In addition, we computed the results of this example by hand and see that they agree with the output of the classifier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'red', 'red', 'blue']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test case\n",
    "\n",
    "X_train = [[\"this\", \"is\", \"an\", \"entry\"], \n",
    "          [\"this\", \"is\", \"too\"], \n",
    "          [\"so\", \"is\", \"this\"]]\n",
    "\n",
    "y_train = [\"red\",\n",
    "          \"blue\",\n",
    "          \"red\"]\n",
    "\n",
    "X_test = [[\"this\", \"is\", \"an\", \"entry\"],\n",
    "          [\"so\", \"this\", \"is\"],\n",
    "          [\"hello\"],\n",
    "          [\"this\", \"is\", \"too\"]]\n",
    "\n",
    "\n",
    "clf = MultinomialNaiveBayes()\n",
    "clf.train(X_train, y_train)\n",
    "clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf.class_to_index={'red': 0, 'blue': 1}\n",
      "clf.word_to_index={'this': 0, 'is': 1, 'an': 2, 'entry': 3, 'too': 4, 'so': 5}\n",
      "clf.priors=[0.6666666666666666, 0.3333333333333333]\n",
      "clf.theta_hat[0]=[0.23076923076923078, 0.23076923076923078, 0.15384615384615385, 0.15384615384615385, 0.07692307692307693, 0.15384615384615385]\n",
      "[0.23076923076923078, 0.23076923076923078, 0.15384615384615385, 0.15384615384615385, 0.07692307692307693, 0.15384615384615385]\n",
      "clf.theta_hat[1]=[0.2222222222222222, 0.2222222222222222, 0.1111111111111111, 0.1111111111111111, 0.2222222222222222, 0.1111111111111111]\n",
      "[0.2222222222222222, 0.2222222222222222, 0.1111111111111111, 0.1111111111111111, 0.2222222222222222, 0.1111111111111111]\n"
     ]
    }
   ],
   "source": [
    "print(f'{clf.class_to_index=}')\n",
    "print(f'{clf.word_to_index=}')\n",
    "print(f'{clf.priors=}')\n",
    "\n",
    "# The entries in the lists in the next two lines should agree.  \n",
    "print(f'{clf.theta_hat[0]=}')\n",
    "print([3/13, 3/13, 2/13, 2/13, 1/13, 2/13])\n",
    "\n",
    "# The entries in the lists in the next two lines should agree.\n",
    "print(f'{clf.theta_hat[1]=}')\n",
    "print([2/9, 2/9, 1/9, 1/9, 2/9, 1/9])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare our ``MultinomialNaiveBayes`` classifier with the multinomial naive Bayes classifier built into [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).  We do this on the [20 Newsgroups](https://scikit-learn.org/0.19/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups) text classification benchmark dataset (see also [here](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)).  We see that our ``MultinomialNaiveBayes`` classifier achieves $0.774$ classification accuracy on this dataset while the builtin sklearn ``MultinomialNB`` achieves an accuracy of $0.772$ (this difference is likely due to some difference in preprocssing).\n",
    "\n",
    "For our use of the ``MultinomialNaiveBayes`` classifier, we somewhat mimick the preprocessing of sklearns [``CountVectorizer``](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class.  In particular, we split words at punctuation and whitespace, remove length one words (``text_preprocess`` below), and remove words that only occur once (``filter_min_count`` below).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "#categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  #categories=categories, \n",
    "                                  shuffle=True, \n",
    "                                  random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-38\" type=\"checkbox\" ><label for=\"sk-estimator-id-38\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()), (&#x27;clf&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-39\" type=\"checkbox\" ><label for=\"sk-estimator-id-39\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7728359001593202"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 #categories=categories, \n",
    "                                 shuffle=True, \n",
    "                                 random_state=42)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "\n",
    "def text_preprocess(s):\n",
    "    # Remove punctuation and replace with whitespace\n",
    "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    s = s.translate(translator)\n",
    "    # lowercase, remove and split at whitespace\n",
    "    s = s.lower().strip().split()\n",
    "    # remove words of length 1 -- done in 'CountVectorizer' in sklearn.\n",
    "    return [c for c in s if len(c) > 1]\n",
    "\n",
    "def filter_min_count(X, min_count):\n",
    "    count = collections.Counter()\n",
    "    for x in X:\n",
    "        count += collections.Counter(x)\n",
    "    remove = set()\n",
    "    for k,v in count.items():\n",
    "        if v < min_count:\n",
    "            remove.add(k)\n",
    "    res = []\n",
    "    for x in X:\n",
    "        current = []\n",
    "        for c in x:\n",
    "            if c in remove:\n",
    "                continue\n",
    "            current.append(c)\n",
    "        res.append(current)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7754912373871482"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = [text_preprocess(x) for x in twenty_train.data]\n",
    "y_train = twenty_train.target\n",
    "\n",
    "my_clf = MultinomialNaiveBayes()\n",
    "my_clf.train(X_train, y_train)\n",
    "X_test = filter_min_count([text_preprocess(doc) for doc in twenty_test.data], 2)\n",
    "#X_test = [text_preprocess(doc) for doc in twenty_test.data]\n",
    "y_test = twenty_test.target\n",
    "\n",
    "predicted = my_clf.predict(X_test)\n",
    "np.mean(predicted == y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
