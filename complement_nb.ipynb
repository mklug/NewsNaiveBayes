{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we present the accuracy of a further variation of the naive Bayes classifier introduced by [Rennie, Shih, Teevan, and Karger](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf) which is implemented in sklearn as [``ComplementNB``](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html).  The cell below we show that this model outperforms both the Bernoulli and multinomial variants of the naive Bayes classifier on the task of text classification for the [20 Newsgroups dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) (although we are just using these models out-of-the-box and not tuning the smoothing parameter or modifying the preprocessing).  We then offer a derivation of the complement naive Bayes classifier by way of the one-versus-rest transformation to the multinomial naive Bayes classifier in the binary case.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of BernoulliNB() = 0.6307753584705258\n",
      "accuracy of MultinomialNB() = 0.7728359001593202\n",
      "accuracy of ComplementNB() = 0.8267392458842273\n",
      "accuracy of ComplementNB(norm=True) = 0.8080191184280403\n",
      "accuracy of OneVsRestClassifier(estimator=BernoulliNB()) = 0.6583908656399363\n",
      "accuracy of OneVsRestClassifier(estimator=MultinomialNB()) = 0.7931492299522039\n",
      "accuracy of OneVsRestClassifier(estimator=ComplementNB()) = 0.7930164630908125\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  shuffle=True, \n",
    "                                  random_state=42)\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test',\n",
    "                                 shuffle=True, \n",
    "                                 random_state=42)\n",
    "\n",
    "X_train = twenty_train.data\n",
    "y_train = twenty_train.target\n",
    "X_test  = twenty_test.data\n",
    "y_test  = twenty_test.target\n",
    "\n",
    "models = [BernoulliNB(),\n",
    "          MultinomialNB(),\n",
    "          ComplementNB(),\n",
    "          ComplementNB(norm=True),\n",
    "          OneVsRestClassifier(BernoulliNB()),\n",
    "          OneVsRestClassifier(MultinomialNB()),\n",
    "          OneVsRestClassifier(ComplementNB())] # It does not really make sense to do this lasat one.\n",
    "\n",
    "for model in models:\n",
    "    clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('clf', model)\n",
    "    ])\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f'accuracy of {model} = {np.mean(clf.predict(X_test) == y_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "X = np.array([\n",
    "    [10, 10],\n",
    "    [8, 10],\n",
    "    [-5, 5.5],\n",
    "    [-5.4, 5.5],\n",
    "    [-20, -20],\n",
    "    [-15, -20]\n",
    "])\n",
    "y = np.array([0, 0, 1, 1, 2, 2])\n",
    "clf = OneVsRestClassifier(SVC()).fit(X, y)\n",
    "clf.predict([[-19, -20], [9, 9], [-5, 5]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we give a brief derivation of the complement naive Bayes classifier assuming familiarity with the usual multinomial naive Bayes classifier.  Many classifiers take the form \n",
    "$$\n",
    "h(x) = \\arg\\max_c f_c(x)\n",
    "$$\n",
    "where the functions $f_c(x)$ are called discrinant functions.  In the case of a binary classification problem, the classes are often labeled with $+$ and $-$ in which case, \n",
    "$$\n",
    "h(x) = \\arg\\max_{\\ast \\in \\{+,-\\}} f_{\\ast}(x) = \\text{sign}(f_{+}(x) - f_{-}(x))\n",
    "$$\n",
    "\n",
    "Some learning algorithms (for example support vector machines) only apply directly to binary classification problems.  One way of obtaining a clasifier for a multiclass problem using such binary classifiers is from the [one-versus-rest](https://en.wikipedia.org/wiki/Multiclass_classification) transfromation to binary.  Assume that we have a response variable $C$ taking on classes $c_1,...,c_K$.  Suppose that we pick a class $c$ and identify all other classes together as another class $\\tilde{c}$.  We then train a binary classifier on the data with the labelings $c$ and $\\tilde{c}$ that returns a discriminant function $f_c(x)$ that when, positive indicates that the binary classifier believes we are in class $c$ with maginitude indicating degree of certainty, and similarly with negative values of the discrimant meaning the binary classifier believes we are not in $c$.  Then by using this learning algorithm on each class $c$, we obtain a discriminant for each class $f_c$ and then, the one-versus-rest classifier is given as \n",
    "$$\n",
    "h_{\\text{one-versus-rest}}(x) = \\arg\\max_{c} f_c(x)\n",
    "$$\n",
    "\n",
    "The binary classification rule we use is the mulitnomial naive Bayes classifier where now, instead of the assumption that the features $X$ conditioned on each class follow a multinomial distribution, we now make the stronger assumption that the features $X$ conditioned on any class or its complement follow a multinomial distribution.  The discriminant functions that result (after the usual application of Bayes' rule, removing terms completely determined by $X$ and taking the logarithm) is \n",
    "$$\n",
    "f_c(x) = \\log(P(C=c)) + \\sum_i x_i \\log(p_{ci}) - \\log(1 - P(C=c)) - \\sum_i x_i \\log(p_{\\tilde{c}i})\n",
    "$$\n",
    "where the terms $p_{ci}$ and $p_{\\tilde{c}i}$ are the parameters of the respective multinomial distributions.  \n",
    "\n",
    "We note that the term $\\log(1- P(C=c))$ is not included by [Rennie, Shih, Teevan, and Karger](https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf) (equation 7).  This is harmless as this number is typically very small.  Indeed ignoring all of the prior terms (sometimes called using a uniform prior) typically makes little difference as many of the $p_{ci}$ are small and so the sums dominate.  \n",
    "\n",
    "The complement naive Bayes classifier is then derived by also ignoring the \"in-class\" terms $\\sum_i x_i \\log(p_{ci})$ in the discriminant.  One possible explanation for the improvement of performance by the complement naive bayes model is the estimates of the $p_{tilde{c}i}$ parameters are typically done using more data than the $p_{ci}$ and therefore complement naive Bayes might have less of an issue with bias.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
